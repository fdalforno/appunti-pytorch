{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro\n",
    "=======\n",
    "Pytorch ha al suo interno dei moduli per creare agevolmente delle reti neuronali e per leggere i dataset il modulo per creare le nn è **torch.nn** mentre le classi per utilizzare i dataset sono **Dataset** e **DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_samples = 5000\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X.astype('float32')\n",
    "y = y.astype('int64')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_samples, test_size=10000)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pyplot.imshow(X_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valore minimo 0 massimo 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Valore minimo {0} massimo {1}\".format(y_train.min(), y_train.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch usa come struttura base il tensore *torch.tensor* dobbiamo quindi convertire l'array numpy in un tensore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stai usando la versione di pytorch 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Stai usando la versione di pytorch {0}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, X_test, y_test = map(\n",
    "    torch.from_numpy , (X_train, y_train, X_test, y_test)\n",
    ")\n",
    "\n",
    "n, c = x_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network da zero\n",
    "=======\n",
    "\n",
    "mi vado a costruire una rete neuronale da zero senza l'uso del modulo **torch.nn** mi vado a creare i pesi con una inizializzazione random utilizzando l'inizializzaione di Xavier \n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{\\sqrt{n}}\n",
    "\\end{equation*}\n",
    "\n",
    "sia il parametro weights che il parametro bias devono avere impostato reqires_grad, questo fa si che pytorch si vada a segnare tutte le operazioni fatte sul tensore e calcolerà in automatico il gradiente nella fase di back-propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Nota:</b> il parametro weights ha impostato requires_grad dopo l'inizializzazione in quanto non vogliamo che l'inizializzazione stessa finisca nel calcolo del gradiente\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ci creiamo la funzione di softmax e il forward del modello, la parte di backpropagation viene fatta in automatico da pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.4570, -0.7388, -2.5455, -2.8431, -1.4866, -7.2059, -5.1740, -2.6367,\n",
      "        -3.0171, -3.8669], grad_fn=<SelectBackward>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch size\n",
    "\n",
    "xb = x_train[0:bs]  # a mini-batch from x\n",
    "preds = model(xb)  # predictions\n",
    "preds[0], preds.shape\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nella cella superiore abbiamo usato un nuovo pezzo *torch.nn.functional* questo modulo contiene tutte le funzioni che ci servono per calcolare la funzione di costo (loss) nel nostro caso la cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6896, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1094)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr = 0.01  # learning rate\n",
    "epochs = 10  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        #set_trace()\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, grad_fn=<NllLossBackward>) tensor(0.8841)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(X_test), y_test), accuracy(model(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sistemiamo il codice con nn.Module\n",
    "=====\n",
    "\n",
    "Utilizzo il modulo nn.Module per semplificare la fase di training, creo una classe che estende nn.Module e nel costruttore ```__init__``` imposto tutti i pesi su cui il modulo andrà a lavorare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo istanziare l'oggetto appena creato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_Logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora possiamo calcolare il costo come abbiamo fatto prima, possiamo utilizzare il modello come una funzione. Verrà chiamato il metodo forward automaticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8963, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prima abbiamo aggiornato i pesi uno per uno e abbiamo azzerato il gradiente a mano, ora possiamo sfruttare il modello per fare questo in maniera quasi automatica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1585, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear\n",
    "=====\n",
    "\n",
    "Andiamo a semplificare ancora il modello andando ad utilizzare nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4743, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1611, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo optim\n",
    "====\n",
    "\n",
    "Pytorch ha un modulo per la gestione dell'ottimizzazione del modello invece di andare ad aggiustare i pesi a mano ci penserà l'ottimizzatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4136, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1704, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "model = Mnist_Logistic()\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "print(loss_func(model(xb), yb))\n",
    "                \n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()    \n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizziamo l'oggetto dataset\n",
    "====\n",
    "\n",
    "PyTorch ha una classe astratta Dataset. Un dataset può essere qualsiasi cosa che ha una funzione ```__len__ ``` per conoscere quanti elementi contiene e una funzione ```__getitem__``` per estrarre un elemento.\n",
    "\n",
    "Andremo ad utilizzare TensorDataset che prende in ingresso due tensori e li combina in un unico sono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = train_ds[i*bs : i*bs+bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
